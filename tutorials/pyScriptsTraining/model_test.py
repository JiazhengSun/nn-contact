from __future__ import print_function
import scipy.io as sio
import numpy as np
from keras.models import load_model
import csv
import os

mpath = '/Users/jiazhengsun/Desktop/nn-contact/data/NN-contact-force/keras_model/3D/'
model = load_model(os.path.join(mpath,'rect_r3_3D_sym.h5'))

x_test_1 = [[-1.93004, 1.93004, -0.511833, 0.692334, 0.0538576, -0.81607, 1.53919,-2.18402, 2.01191], 
			[1.25728, 1.13107, -1.13107, 0.182227, 0.691963, -0.175838, 3.41511, -5.99775, -0.846054],
			[0.246902, -2.09713, 2.09713, -0.69256, 0.14331, 0.604811, -4.66946, 0.895696, -0.0151988],
			[-2.12687, 2.12687, -0.192541, 0.511687, -0.07551, 0.902735, 1.32739, -1.57741, 3.24697],
			[-0.235922, -0.235922, 1.55789, 0.384338, -0.43193, 0.553732, 2.83865, -2.01604, -2.17844]]

x_test_1 = np.array(x_test_1)
print(model.predict(x_test_1, batch_size=1))

# for i in range(4):
# 	Wname = 'W' + str(i)
# 	Bname = 'B' + str(i)
# 	Wmat = model.layers[i].get_weights()[0]
# 	#Wmat = np.transpose(Wmat)
# 	print(Wname)
# 	for i in range(len(Wmat)):
# 		print(Wmat[i])
# 	#Bmat = model.layers[i].get_weights()[1]
# 	#Bmat = np.transpose(Bmat)
# 	#print(Bmat.shape)

# Original data
# -1.93004, 1.93004, -0.511833, 0.692334, 0.0538576, -0.81607, 1.53919,-2.18402, 2.01191, -217.572, -691, 6.95441E-05
# 1.25728, 1.13107, -1.13107, 0.182227, 0.691963, -0.175838, 3.41511, -5.99775, -0.846054, -1567.38, 437.869, 0.000119291	
# 0.246902, -2.09713, 2.09713, -0.69256, 0.14331, 0.604811, -4.66946, 0.895696, -0.0151988, 124.717, -104.373, -8.56153	
# -2.12687, 2.12687, -0.192541, 0.511687, -0.07551, 0.902735, 1.32739, -1.57741, 3.24697		-178.366	-788.702	-24.6237	
# -0.235922, -0.235922, 1.55789, 0.384338, -0.43193, 0.553732, 2.83865, -2.01604, -2.17844	-1008.01	895.091	-10.9528	

#r3 Result:
# [[-2.2157016e+00 -7.0726523e+00  7.4795989e-04]
#  [-1.5470692e+01  4.6916561e+00  8.6386604e-03]
#  [ 1.4865875e+00 -5.2017045e-01 -1.2168298e-01]
#  [-1.8900229e+00 -8.0110998e+00 -1.4450493e-01]
#  [-1.0462066e+01  9.0114565e+00  2.6248695e-02]]

#Tiny-dnn result given same input

# -2.2157 -7.07265 0.000747957  
# -15.4707 4.69165 0.00863866  
# 1.48659 -0.52017 -0.121683  
# -1.89002 -8.0111 -0.144505  
# -10.462 9.01139 0.0262461 